{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import baseRNN\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "NUM_SAMPLES = 10000\n",
    "imdbDataset = load_dataset(\"stanfordnlp/imdb\")\n",
    "\n",
    "embeddingsFilepath = '/Users/josep/Desktop/Self/Learning/NLP/RNN/data/glove.6B.300d.txt'\n",
    "\n",
    "# helper functions\n",
    "def read_corpus(dataset):\n",
    "    files = dataset[\"train\"][\"text\"][:NUM_SAMPLES]\n",
    "    return [[START_TOKEN] + [re.sub(r'[^\\w]', '', w.lower()) for w in f.split(\" \")] + [END_TOKEN] for f in files]\n",
    "\n",
    "\n",
    "def embedding_for_vocab(filepath, words, dimensions):\n",
    "    vocab_size = len(words)\n",
    "    embeddings = np.zeros((vocab_size, dimensions))\n",
    "\n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in words.keys():\n",
    "                index = words[word]\n",
    "                embeddings[index] = np.array(vector)[:dimensions]\n",
    "    return embeddings\n",
    "\n",
    "imdbCorpus = read_corpus(imdbDataset)\n",
    "\n",
    "corpusWords = [y for x in imdbCorpus for y in x]\n",
    "corpusWords = list(set(corpusWords))\n",
    "word2ind={}\n",
    "for i in range(len(corpusWords)+1):\n",
    "    word2ind[corpusWords[i-1]] = i\n",
    "word2ind['<PAD>'] = 0\n",
    "embeddings = embedding_for_vocab(embeddingsFilepath, word2ind, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testText = np.array(imdbCorpus[0])\n",
    "mapper = np.vectorize(word2ind.get)\n",
    "result = mapper(testText)\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imdbCorpus[0])%3\n",
    "\n",
    "len(imdbCorpus[0][:-2])%3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "testRNN = baseRNN.neuralNet(embeddings=embeddings, word2ind=word2ind, outputActivation='softmax',\n",
    "                            hiddenLayerShapes=[100,100,100], hiddenLayerActivations=['relu', 'relu', 'relu'],\n",
    "                            lossFunction='crossEntropyLoss', learningRate=.0005, epochs=1, batchSize=5,\n",
    "                            adam=True, clipVal=1, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overfitCorpus = [imdbCorpus[0][0:20]] * 10\n",
    "# # overfitCorpus = [overfitCorpus, overfitCorpus]\n",
    "testCorpus = imdbCorpus\n",
    "# overfitCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch #1 - max 313 words\n",
      "Loss: 7.007187875386797\n",
      "********************************************\n",
      "\n",
      "Batch #2 - max 482 words\n",
      "Loss: 6.821120737864705\n",
      "********************************************\n",
      "\n",
      "Batch #3 - max 362 words\n",
      "Loss: 6.882234510939532\n",
      "********************************************\n",
      "\n",
      "Batch #4 - max 890 words\n",
      "Loss: 6.944576375443166\n",
      "********************************************\n",
      "\n",
      "Batch #5 - max 327 words\n",
      "Loss: 6.831435084983322\n",
      "********************************************\n",
      "\n",
      "Batch #6 - max 371 words\n",
      "Loss: 6.77104223989011\n",
      "********************************************\n",
      "\n",
      "Batch #7 - max 387 words\n",
      "Loss: 6.782372356400827\n",
      "********************************************\n",
      "\n",
      "Batch #8 - max 298 words\n",
      "Loss: 6.247155458119795\n",
      "********************************************\n",
      "\n",
      "Batch #9 - max 839 words\n",
      "Loss: 6.819218237508209\n",
      "********************************************\n",
      "\n",
      "Batch #10 - max 159 words\n",
      "Loss: 6.15005121638279\n",
      "********************************************\n",
      "\n",
      "Batch #11 - max 464 words\n",
      "Loss: 6.7579712978724835\n",
      "********************************************\n",
      "\n",
      "Batch #12 - max 460 words\n",
      "Loss: 6.58021084723437\n",
      "********************************************\n",
      "\n",
      "Batch #13 - max 328 words\n",
      "Loss: 6.182519972855686\n",
      "********************************************\n",
      "\n",
      "Batch #14 - max 772 words\n",
      "Loss: 6.6691793067485685\n",
      "********************************************\n",
      "\n",
      "Batch #15 - max 366 words\n",
      "Loss: 6.75628290307408\n",
      "********************************************\n",
      "\n",
      "Batch #16 - max 353 words\n",
      "Loss: 6.435039737104611\n",
      "********************************************\n",
      "\n",
      "Batch #17 - max 631 words\n",
      "Loss: 6.7672722317787315\n",
      "********************************************\n",
      "\n",
      "Batch #18 - max 591 words\n",
      "Loss: 6.443511130680676\n",
      "********************************************\n",
      "\n",
      "Batch #19 - max 867 words\n",
      "Loss: 6.780354812201508\n",
      "********************************************\n",
      "\n",
      "Batch #20 - max 522 words\n",
      "Loss: 6.6930365299828365\n",
      "********************************************\n",
      "\n",
      "Batch #21 - max 257 words\n",
      "Loss: 6.454794453327917\n",
      "********************************************\n",
      "\n",
      "Batch #22 - max 278 words\n",
      "Loss: 6.309312359752111\n",
      "********************************************\n",
      "\n",
      "Batch #23 - max 502 words\n",
      "Loss: 6.767918767437887\n",
      "********************************************\n",
      "\n",
      "Batch #24 - max 333 words\n",
      "Loss: 6.403086900148701\n",
      "********************************************\n",
      "\n",
      "Batch #25 - max 324 words\n",
      "Loss: 6.419389525834022\n",
      "********************************************\n",
      "\n",
      "Batch #26 - max 865 words\n",
      "Loss: 6.584240155408357\n",
      "********************************************\n",
      "\n",
      "Batch #27 - max 308 words\n",
      "Loss: 6.5052608298286385\n",
      "********************************************\n",
      "\n",
      "Batch #28 - max 263 words\n",
      "Loss: 6.630451396132294\n",
      "********************************************\n",
      "\n",
      "Batch #29 - max 596 words\n",
      "Loss: 6.994387732987051\n",
      "********************************************\n",
      "\n",
      "Batch #30 - max 428 words\n",
      "Loss: 6.908728893126425\n",
      "********************************************\n",
      "\n",
      "Batch #31 - max 193 words\n",
      "Loss: 6.785530306787902\n",
      "********************************************\n",
      "\n",
      "Batch #32 - max 254 words\n",
      "Loss: 6.48047780761487\n",
      "********************************************\n",
      "\n",
      "Batch #33 - max 260 words\n",
      "Loss: 6.566292445015161\n",
      "********************************************\n",
      "\n",
      "Batch #34 - max 381 words\n",
      "Loss: 6.707092824988445\n",
      "********************************************\n",
      "\n",
      "Batch #35 - max 266 words\n",
      "Loss: 6.663014388581193\n",
      "********************************************\n",
      "\n",
      "Batch #36 - max 333 words\n",
      "Loss: 6.750000646241726\n",
      "********************************************\n",
      "\n",
      "Batch #37 - max 915 words\n",
      "Loss: 6.850024108491055\n",
      "********************************************\n",
      "\n",
      "Batch #38 - max 228 words\n",
      "Loss: 6.566949261222899\n",
      "********************************************\n",
      "\n",
      "Batch #39 - max 216 words\n",
      "Loss: 6.457004854448454\n",
      "********************************************\n",
      "\n",
      "Batch #40 - max 908 words\n",
      "Loss: 6.920451650842669\n",
      "********************************************\n",
      "\n",
      "Batch #41 - max 376 words\n",
      "Loss: 6.741228513436036\n",
      "********************************************\n",
      "\n",
      "Batch #42 - max 652 words\n",
      "Loss: 6.180307878403551\n",
      "********************************************\n",
      "\n",
      "Batch #43 - max 457 words\n",
      "Loss: 6.5172569523871875\n",
      "********************************************\n",
      "\n",
      "Batch #44 - max 498 words\n",
      "Loss: 6.724703975232254\n",
      "********************************************\n",
      "\n",
      "Batch #45 - max 158 words\n",
      "Loss: 6.824410895924977\n",
      "********************************************\n",
      "\n",
      "Batch #46 - max 306 words\n",
      "Loss: 6.753273295068546\n",
      "********************************************\n",
      "\n",
      "Batch #47 - max 1001 words\n",
      "Loss: 7.148307708239377\n",
      "********************************************\n",
      "\n",
      "Batch #48 - max 537 words\n",
      "Loss: 6.706255043141826\n",
      "********************************************\n",
      "\n",
      "Batch #49 - max 152 words\n",
      "Loss: 6.17519901415094\n",
      "********************************************\n",
      "\n",
      "Batch #50 - max 838 words\n",
      "Loss: 6.4629778990627385\n",
      "********************************************\n",
      "\n",
      "Batch #51 - max 768 words\n",
      "Loss: 6.592655782210821\n",
      "********************************************\n",
      "\n",
      "Batch #52 - max 411 words\n",
      "Loss: 6.6702159428345045\n",
      "********************************************\n",
      "\n",
      "Batch #53 - max 366 words\n",
      "Loss: 6.626893087375842\n",
      "********************************************\n",
      "\n",
      "Batch #54 - max 856 words\n",
      "Loss: 6.850417833774855\n",
      "********************************************\n",
      "\n",
      "Batch #55 - max 434 words\n",
      "Loss: 6.714397286544185\n",
      "********************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testRNN.trainModel(imdbCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<START>', 'my', 'name', 'is', 'video', 'what', 'is', 'yours', 'the', 'to', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the']\n"
     ]
    }
   ],
   "source": [
    "input = ['<START>', 'my', 'name', 'is', 'video', 'what', 'is', 'yours']\n",
    "\n",
    "# input = ['<START>', 'because', 'of', 'all', 'the', 'controversy']\n",
    "\n",
    "output = testRNN.generateOutput(input, 10)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<START>', 'i', 'rented', 'i', 'am', 'curiousyellow', 'from', 'my', 'video', 'store', 'because', 'of', 'all', 'the', 'controversy', 'that', 'surrounded', 'it', 'when', 'it', 'was', 'first', 'released', 'in', '1967', 'i', 'also', 'heard', 'that', 'at', 'first', 'it', 'was', 'seized', 'by', 'us', 'customs', 'if', 'it', 'ever', 'tried', 'to', 'enter', 'this', 'country', 'therefore', 'being', 'a', 'fan', 'of', 'films', 'considered', 'controversial', 'i', 'really', 'had', 'to', 'see', 'this', 'for', 'myselfbr', 'br', 'the', 'plot', 'is', 'centered', 'around', 'a', 'young', 'swedish', 'drama', 'student', 'named', 'lena', 'who', 'wants', 'to', 'learn', 'everything', 'she', 'can', 'about', 'life', 'in', 'particular', 'she', 'wants', 'to', 'focus', 'her', 'attentions', 'to', 'making', 'some', 'sort', 'of', 'documentary', 'on', 'what', 'the', 'average', 'swede', 'thought', 'about', 'certain', 'political', 'issues', 'such', 'as', 'the', 'vietnam', 'war', 'and', 'race', 'issues', 'in', 'the', 'united', 'states', 'in', 'between', 'asking', 'politicians', 'and', 'ordinary', 'denizens', 'of', 'stockholm', 'about', 'their', 'opinions', 'on', 'politics', 'she', 'has', 'sex', 'with', 'her', 'drama', 'teacher', 'classmates', 'and', 'married', 'menbr', 'br', 'what', 'kills', 'me', 'about', 'i', 'am', 'curiousyellow', 'is', 'that', '40', 'years', 'ago', 'this', 'was', 'considered', 'pornographic', 'really', 'the', 'sex', 'and', 'nudity', 'scenes', 'are', 'few', 'and', 'far', 'between', 'even', 'then', 'its', 'not', 'shot', 'like', 'some', 'cheaply', 'made', 'porno', 'while', 'my', 'countrymen', 'mind', 'find', 'it', 'shocking', 'in', 'reality', 'sex', 'and', 'nudity', 'are', 'a', 'major', 'staple', 'in', 'swedish', 'cinema', 'even', 'ingmar', 'bergman', 'arguably', 'their', 'answer', 'to', 'good', 'old', 'boy', 'john', 'ford', 'had', 'sex', 'scenes', 'in', 'his', 'filmsbr', 'br', 'i', 'do', 'commend', 'the', 'filmmakers', 'for', 'the', 'fact', 'that', 'any', 'sex', 'shown', 'in', 'the', 'film', 'is', 'shown', 'for', 'artistic', 'purposes', 'rather', 'than', 'just', 'to', 'shock', 'people', 'and', 'make', 'money', 'to', 'be', 'shown', 'in', 'pornographic', 'theaters', 'in', 'america', 'i', 'am', 'curiousyellow', 'is', 'a', 'good', 'film', 'for', 'anyone', 'wanting', 'to', 'study', 'the', 'meat', 'and', 'potatoes', 'no', 'pun', 'intended', 'of', 'swedish', 'cinema', 'but', 'really', 'this', 'film', 'doesnt', 'have', 'much', 'of', 'a', 'plot', '<END>'], ['<START>', 'i', 'am', 'curious', 'yellow', 'is', 'a', 'risible', 'and', 'pretentious', 'steaming', 'pile', 'it', 'doesnt', 'matter', 'what', 'ones', 'political', 'views', 'are', 'because', 'this', 'film', 'can', 'hardly', 'be', 'taken', 'seriously', 'on', 'any', 'level', 'as', 'for', 'the', 'claim', 'that', 'frontal', 'male', 'nudity', 'is', 'an', 'automatic', 'nc17', 'that', 'isnt', 'true', 'ive', 'seen', 'rrated', 'films', 'with', 'male', 'nudity', 'granted', 'they', 'only', 'offer', 'some', 'fleeting', 'views', 'but', 'where', 'are', 'the', 'rrated', 'films', 'with', 'gaping', 'vulvas', 'and', 'flapping', 'labia', 'nowhere', 'because', 'they', 'dont', 'exist', 'the', 'same', 'goes', 'for', 'those', 'crappy', 'cable', 'shows', 'schlongs', 'swinging', 'in', 'the', 'breeze', 'but', 'not', 'a', 'clitoris', 'in', 'sight', 'and', 'those', 'pretentious', 'indie', 'movies', 'like', 'the', 'brown', 'bunny', 'in', 'which', 'were', 'treated', 'to', 'the', 'site', 'of', 'vincent', 'gallos', 'throbbing', 'johnson', 'but', 'not', 'a', 'trace', 'of', 'pink', 'visible', 'on', 'chloe', 'sevigny', 'before', 'crying', 'or', 'implying', 'doublestandard', 'in', 'matters', 'of', 'nudity', 'the', 'mentally', 'obtuse', 'should', 'take', 'into', 'account', 'one', 'unavoidably', 'obvious', 'anatomical', 'difference', 'between', 'men', 'and', 'women', 'there', 'are', 'no', 'genitals', 'on', 'display', 'when', 'actresses', 'appears', 'nude', 'and', 'the', 'same', 'cannot', 'be', 'said', 'for', 'a', 'man', 'in', 'fact', 'you', 'generally', 'wont', 'see', 'female', 'genitals', 'in', 'an', 'american', 'film', 'in', 'anything', 'short', 'of', 'porn', 'or', 'explicit', 'erotica', 'this', 'alleged', 'doublestandard', 'is', 'less', 'a', 'double', 'standard', 'than', 'an', 'admittedly', 'depressing', 'ability', 'to', 'come', 'to', 'terms', 'culturally', 'with', 'the', 'insides', 'of', 'womens', 'bodies', '<END>'], ['<START>', 'if', 'only', 'to', 'avoid', 'making', 'this', 'type', 'of', 'film', 'in', 'the', 'future', 'this', 'film', 'is', 'interesting', 'as', 'an', 'experiment', 'but', 'tells', 'no', 'cogent', 'storybr', 'br', 'one', 'might', 'feel', 'virtuous', 'for', 'sitting', 'thru', 'it', 'because', 'it', 'touches', 'on', 'so', 'many', 'important', 'issues', 'but', 'it', 'does', 'so', 'without', 'any', 'discernable', 'motive', 'the', 'viewer', 'comes', 'away', 'with', 'no', 'new', 'perspectives', 'unless', 'one', 'comes', 'up', 'with', 'one', 'while', 'ones', 'mind', 'wanders', 'as', 'it', 'will', 'invariably', 'do', 'during', 'this', 'pointless', 'filmbr', 'br', 'one', 'might', 'better', 'spend', 'ones', 'time', 'staring', 'out', 'a', 'window', 'at', 'a', 'tree', 'growingbr', 'br', '', '<END>'], ['<START>', 'this', 'film', 'was', 'probably', 'inspired', 'by', 'godards', 'masculin', 'f√©minin', 'and', 'i', 'urge', 'you', 'to', 'see', 'that', 'film', 'insteadbr', 'br', 'the', 'film', 'has', 'two', 'strong', 'elements', 'and', 'those', 'are', '1', 'the', 'realistic', 'acting', '2', 'the', 'impressive', 'undeservedly', 'good', 'photo', 'apart', 'from', 'that', 'what', 'strikes', 'me', 'most', 'is', 'the', 'endless', 'stream', 'of', 'silliness', 'lena', 'nyman', 'has', 'to', 'be', 'most', 'annoying', 'actress', 'in', 'the', 'world', 'she', 'acts', 'so', 'stupid', 'and', 'with', 'all', 'the', 'nudity', 'in', 'this', 'filmits', 'unattractive', 'comparing', 'to', 'godards', 'film', 'intellectuality', 'has', 'been', 'replaced', 'with', 'stupidity', 'without', 'going', 'too', 'far', 'on', 'this', 'subject', 'i', 'would', 'say', 'that', 'follows', 'from', 'the', 'difference', 'in', 'ideals', 'between', 'the', 'french', 'and', 'the', 'swedish', 'societybr', 'br', 'a', 'movie', 'of', 'its', 'time', 'and', 'place', '210', '<END>'], ['<START>', 'oh', 'brotherafter', 'hearing', 'about', 'this', 'ridiculous', 'film', 'for', 'umpteen', 'years', 'all', 'i', 'can', 'think', 'of', 'is', 'that', 'old', 'peggy', 'lee', 'songbr', 'br', 'is', 'that', 'all', 'there', 'is', 'i', 'was', 'just', 'an', 'early', 'teen', 'when', 'this', 'smoked', 'fish', 'hit', 'the', 'us', 'i', 'was', 'too', 'young', 'to', 'get', 'in', 'the', 'theater', 'although', 'i', 'did', 'manage', 'to', 'sneak', 'into', 'goodbye', 'columbus', 'then', 'a', 'screening', 'at', 'a', 'local', 'film', 'museum', 'beckoned', '', 'finally', 'i', 'could', 'see', 'this', 'film', 'except', 'now', 'i', 'was', 'as', 'old', 'as', 'my', 'parents', 'were', 'when', 'they', 'schlepped', 'to', 'see', 'itbr', 'br', 'the', 'only', 'reason', 'this', 'film', 'was', 'not', 'condemned', 'to', 'the', 'anonymous', 'sands', 'of', 'time', 'was', 'because', 'of', 'the', 'obscenity', 'case', 'sparked', 'by', 'its', 'us', 'release', 'millions', 'of', 'people', 'flocked', 'to', 'this', 'stinker', 'thinking', 'they', 'were', 'going', 'to', 'see', 'a', 'sex', 'filminstead', 'they', 'got', 'lots', 'of', 'closeups', 'of', 'gnarly', 'repulsive', 'swedes', 'onstreet', 'interviews', 'in', 'bland', 'shopping', 'malls', 'asinie', 'political', 'pretensionand', 'feeble', 'whocares', 'simulated', 'sex', 'scenes', 'with', 'saggy', 'pale', 'actorsbr', 'br', 'cultural', 'icon', 'holy', 'grail', 'historic', 'artifactwhatever', 'this', 'thing', 'was', 'shred', 'it', 'burn', 'it', 'then', 'stuff', 'the', 'ashes', 'in', 'a', 'lead', 'boxbr', 'br', 'elite', 'esthetes', 'still', 'scrape', 'to', 'find', 'value', 'in', 'its', 'boring', 'pseudo', 'revolutionary', 'political', 'spewingsbut', 'if', 'it', 'werent', 'for', 'the', 'censorship', 'scandal', 'it', 'would', 'have', 'been', 'ignored', 'then', 'forgottenbr', 'br', 'instead', 'the', 'i', 'am', 'blank', 'blank', 'rhythymed', 'title', 'was', 'repeated', 'endlessly', 'for', 'years', 'as', 'a', 'titilation', 'for', 'porno', 'films', 'i', 'am', 'curious', 'lavender', '', 'for', 'gay', 'films', 'i', 'am', 'curious', 'black', '', 'for', 'blaxploitation', 'films', 'etc', 'and', 'every', 'ten', 'years', 'or', 'so', 'the', 'thing', 'rises', 'from', 'the', 'dead', 'to', 'be', 'viewed', 'by', 'a', 'new', 'generation', 'of', 'suckers', 'who', 'want', 'to', 'see', 'that', 'naughty', 'sex', 'film', 'that', 'revolutionized', 'the', 'film', 'industrybr', 'br', 'yeesh', 'avoid', 'like', 'the', 'plagueor', 'if', 'you', 'must', 'see', 'it', '', 'rent', 'the', 'video', 'and', 'fast', 'forward', 'to', 'the', 'dirty', 'parts', 'just', 'to', 'get', 'it', 'over', 'withbr', 'br', '', '<END>']]\n"
     ]
    }
   ],
   "source": [
    "print(testCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = 'C:/Users/josep/Desktop/Self/Learning/NLP/Basic NN/Model/'\n",
    "\n",
    "for layerName in testRNN.layers.keys():\n",
    "    currLayer = testRNN.layers[layerName]\n",
    "    layerWeights = currLayer.layerWeights\n",
    "    np.save(filePath + layerName + '_layerWeights.npy', layerWeights)\n",
    "    \n",
    "    if currLayer.rnn:\n",
    "        timeWeights = currLayer.timeWeights\n",
    "        np.save(filePath + layerName + '_timeWeights.npy', timeWeights)\n",
    "    bias = currLayer.bias\n",
    "    np.save(filePath + layerName + '_bias.npy', bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
